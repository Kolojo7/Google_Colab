{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyMF6be12vfoxNrPN+IQdSSg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kolojo7/Google_Colab/blob/main/asl_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "sWITR1YWgdeU",
        "outputId": "f6c954dc-00e5-418c-c8a0-e2370d7ced4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "img should be PIL Image. Got <class 'numpy.ndarray'>",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-43fd1d7cb000>\u001b[0m in \u001b[0;36m<cell line: 130>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Set the model to training mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0macc_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-43fd1d7cb000>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Ensure the image is in the correct tensor type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    711\u001b[0m         \"\"\"\n\u001b[1;32m    712\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 713\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhflip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    714\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mhflip\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0m_log_api_usage_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhflip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF_pil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhflip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mF_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhflip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/_functional_pil.py\u001b[0m in \u001b[0;36mhflip\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mhflip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_is_pil_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"img should be PIL Image. Got {type(img)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFLIP_LEFT_RIGHT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: img should be PIL Image. Got <class 'numpy.ndarray'>"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load in our data from CSV files\n",
        "train_df = pd.read_csv(\"/content/drive/MyDrive/colab_data/data/asl_data/sign_mnist_train.csv\")\n",
        "valid_df = pd.read_csv(\"/content/drive/MyDrive/colab_data/data/asl_data/sign_mnist_valid.csv\")\n",
        "\n",
        "# Separate out our target values\n",
        "y_train = train_df['label']\n",
        "y_valid = valid_df['label']\n",
        "del train_df['label']\n",
        "del valid_df['label']\n",
        "\n",
        "# Separate out our image vectors\n",
        "x_train = train_df.values\n",
        "x_valid = valid_df.values\n",
        "\n",
        "num_classes = 24\n",
        "\n",
        "# Convert Pandas DataFrame to PyTorch tensor\n",
        "y_train_data = torch.tensor(y_train.values, dtype=torch.long)\n",
        "y_valid_data = torch.tensor(y_valid.values, dtype=torch.long)\n",
        "\n",
        "# Normalize our image data\n",
        "x_train = x_train / 255.\n",
        "x_valid = x_valid / 255.\n",
        "\n",
        "# Reshape the image data for the convolutional network (correct shape for Conv2d)\n",
        "x_train = x_train.reshape(-1, 1, 28, 28)\n",
        "x_valid = x_valid.reshape(-1, 1, 28, 28)\n",
        "\n",
        "# Define data augmentation transformations for training\n",
        "data_transforms = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(p=0.5),  # Random horizontal flip with 50% probability\n",
        "    transforms.RandomRotation(degrees=15),   # Random rotation within -15 to 15 degrees\n",
        "    transforms.ToTensor(),                   # Convert the image to a PyTorch tensor\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize the image (mean=0.5, std=0.5)\n",
        "])\n",
        "\n",
        "# For validation, we only need to convert to tensor and normalize (no augmentation)\n",
        "valid_transforms = transforms.Compose([\n",
        "    transforms.ToTensor(),                   # Convert to tensor\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize with the same mean and std\n",
        "])\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data, labels, transform=None):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.data[idx]\n",
        "        label = self.labels[idx]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        image = image.type(torch.float32)  # Ensure the image is in the correct tensor type\n",
        "        return image, label\n",
        "\n",
        "train_dataset = CustomDataset(data=x_train, labels=y_train_data, transform=data_transforms)\n",
        "valid_dataset = CustomDataset(data=x_valid, labels=y_valid_data, transform=valid_transforms)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=128, shuffle=False)\n",
        "\n",
        "# Define the model (same as before)\n",
        "class CustomCNN(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(CustomCNN, self).__init__()\n",
        "\n",
        "        # Define the layers\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=25, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(25)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        self.conv2 = nn.Conv2d(in_channels=25, out_channels=50, kernel_size=3, stride=1, padding=1)\n",
        "        self.dropout1 = nn.Dropout2d(p=0.2)\n",
        "        self.bn2 = nn.BatchNorm2d(50)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        self.conv3 = nn.Conv2d(in_channels=50, out_channels=100, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(100)\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        self.fc1 = nn.Linear(100 * 3 * 3, 512)\n",
        "        self.dropout2 = nn.Dropout(p=0.3)\n",
        "        self.fc2 = nn.Linear(512, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forward pass through the network\n",
        "        x = torch.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.pool1(x)\n",
        "        x = torch.relu(self.bn2(self.conv2(x)))\n",
        "        x = self.dropout1(x)\n",
        "        x = self.pool2(x)\n",
        "        x = torch.relu(self.bn3(self.conv3(x)))\n",
        "        x = self.pool3(x)\n",
        "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Instantiate the model\n",
        "model = CustomCNN(num_classes)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()  # Equivalent to categorical cross-entropy\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "# Define a metric function to calculate accuracy\n",
        "def calculate_accuracy(outputs, targets):\n",
        "    predicted = outputs.argmax(-1)\n",
        "    correct = (predicted == targets).sum().item()\n",
        "    total = targets.size(0)\n",
        "    return correct / total\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 30\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "    acc_all = []\n",
        "    for images, labels in train_loader:\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        optimizer.zero_grad()  # Zero the gradients\n",
        "        # Backpropagation and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # Get accuracy\n",
        "        acc = calculate_accuracy(outputs, labels)\n",
        "        acc_all.append(acc)\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    acc_val_all = []\n",
        "    with torch.no_grad():\n",
        "        total_correct = 0\n",
        "        total_samples = 0\n",
        "        for images, labels in valid_loader:\n",
        "            outputs = model(images)\n",
        "            loss_val = criterion(outputs, labels)\n",
        "            acc_val = calculate_accuracy(outputs, labels)\n",
        "            acc_val_all.append(acc_val)\n",
        "        acc_avg = np.array(acc_all).mean()\n",
        "        acc_val_avg = np.array(acc_val_all).mean()\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Train loss: {loss:.4f}, Training Accuracy: {acc_avg:.4f},  Validation loss: {loss_val:.4f}, Validation Accuracy: {acc_val_avg:.4f}')\n",
        "\n",
        "# Save the trained model\n",
        "torch.save(model.state_dict(), 'asl_model.pth')\n",
        "\n",
        "# Shutdown the kernel (optional)\n",
        "import IPython\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Load in our data from CSV files\n",
        "train_df = pd.read_csv(\"/content/drive/MyDrive/colab_data/data/asl_data/sign_mnist_train.csv\")\n",
        "valid_df = pd.read_csv(\"/content/drive/MyDrive/colab_data/data/asl_data/sign_mnist_valid.csv\")\n",
        "\n",
        "# Separate out our target values\n",
        "y_train = train_df['label']\n",
        "y_valid = valid_df['label']\n",
        "del train_df['label']\n",
        "del valid_df['label']\n",
        "\n",
        "# Separate out our image vectors\n",
        "x_train = train_df.values\n",
        "x_valid = valid_df.values\n",
        "\n",
        "num_classes = 24\n",
        "\n",
        "# Convert Pandas DataFrame to PyTorch tensor\n",
        "y_train_data = torch.tensor(y_train.values, dtype=torch.long)\n",
        "y_valid_data = torch.tensor(y_valid.values, dtype=torch.long)\n",
        "\n",
        "# Categorically encode y_train and y_valid using one-hot encoding\n",
        "y_train = F.one_hot(y_train_data, num_classes)\n",
        "y_valid = F.one_hot(y_valid_data, num_classes)\n",
        "\n",
        "# Normalize our image data\n",
        "x_train = x_train / 255.\n",
        "x_valid = x_valid / 255.\n",
        "\n",
        "# Reshape the image data for the convolutional network\n",
        "x_train = x_train.reshape(-1,1,28,28)\n",
        "x_valid = x_valid.reshape(-1,1,28,28)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class CustomCNN(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(CustomCNN, self).__init__()\n",
        "\n",
        "        # Define the layers\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=25, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(25)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(in_channels=25, out_channels=50, kernel_size=3, stride=1, padding=1)\n",
        "        self.dropout1 = nn.Dropout2d(p=0.2)\n",
        "        self.bn2 = nn.BatchNorm2d(50)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv3 = nn.Conv2d(in_channels=50, out_channels=100, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(100)\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.fc1 = nn.Linear(100 * 3 * 3, 512)\n",
        "        self.dropout2 = nn.Dropout(p=0.3)\n",
        "        self.fc2 = nn.Linear(512, num_classes)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forward pass through the network\n",
        "        B, C, W, H = x.shape\n",
        "        x = torch.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.pool1(x)\n",
        "        x = torch.relu(self.bn2(self.conv2(x)))\n",
        "        x = self.dropout1(x)\n",
        "        x = self.pool2(x)\n",
        "        x = torch.relu(self.bn3(self.conv3(x)))\n",
        "        x = self.pool3(x)\n",
        "        x = x.reshape(B, -1)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.softmax(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "num_classes = 24  # Replace with the actual number of classes\n",
        "model = CustomCNN(num_classes) # Create an instance of the model\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "# Define data augmentation transformations\n",
        "data_transforms = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),  # Randomly flip images horizontally\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),  # Adjust image color\n",
        "    transforms.RandomAffine(degrees=10, translate=(0.1, 0.1), scale=(0.9, 1.1), shear=10),  # Random affine transformation\n",
        "    transforms.RandomResizedCrop(28, scale=(0.8, 1.0), ratio=(0.75, 1.333)),  # Randomly crop and resize images\n",
        "    transforms.RandomRotation(10),  # Randomly rotate images by up to 10 degrees\n",
        "    transforms.ToTensor(),  # Convert images to tensors\n",
        "])\n",
        "\n",
        "# We dont need to do augmentation for validation data\n",
        "# For validation all we need to do is convert the images into tensor\n",
        "valid_transforms = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data, labels, transform=None):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.data[idx]\n",
        "        label = self.labels[idx]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        #image = image.type(torch.float32)\n",
        "        return torch.tensor(image, dtype=torch.float32), label\n",
        "\n",
        "train_dataset = CustomDataset(data=x_train, labels=y_train_data)\n",
        "valid_dataset = CustomDataset(data=x_valid, labels=y_valid_data)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=128, shuffle=False)\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()  # Equivalent to categorical cross-entropy\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "# Define a metric function to calculate accuracy\n",
        "def calculate_accuracy(outputs, targets):\n",
        "    predicted = outputs.argmax(-1)\n",
        "    correct = (predicted == targets).sum().item()\n",
        "    total = targets.size(0)\n",
        "    return correct / total\n",
        "\n",
        "# Training loop\n",
        "from PIL import Image\n",
        "num_epochs = 40\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "    acc_all = []\n",
        "    for images, labels in train_loader:\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        optimizer.zero_grad()  # Zero the gradients\n",
        "        # Backpropagation and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # Get accuracy\n",
        "        acc = calculate_accuracy(outputs,labels)\n",
        "        acc_all.append(acc)\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    acc_val_all = []\n",
        "    with torch.no_grad():\n",
        "        total_correct = 0\n",
        "        total_samples = 0\n",
        "        for images, labels in valid_loader:\n",
        "            outputs = model(images)\n",
        "            loss_val = criterion(outputs, labels)\n",
        "            acc_val = calculate_accuracy(outputs,labels)\n",
        "            acc_val_all.append(acc_val)\n",
        "        acc_avg = np.array(acc_all).mean()\n",
        "        acc_val_avg = np.array(acc_val_all).mean()\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Train loss: {loss:.4f}, Training Accuracy: {acc_avg:.4f},  Validation loss: {loss_val:.4f}, Validation Accuracy: {acc_val_avg:.4f}')\n",
        "\n",
        "import torch\n",
        "\n",
        "# Assuming 'model' is your trained PyTorch model\n",
        "torch.save(model.state_dict(), '/content/drive/MyDrive/colab_data/data/asl_data/asl_model3.pth')\n",
        "\n",
        "import IPython\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6OmXFnbd7kHc",
        "outputId": "144899a7-68dd-4809-f1d7-174599475fe0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Epoch [1/40], Train loss: 2.6643, Training Accuracy: 0.2990,  Validation loss: 2.8189, Validation Accuracy: 0.4653\n",
            "Epoch [2/40], Train loss: 2.6065, Training Accuracy: 0.6174,  Validation loss: 2.5698, Validation Accuracy: 0.6312\n",
            "Epoch [3/40], Train loss: 2.5710, Training Accuracy: 0.7023,  Validation loss: 2.5233, Validation Accuracy: 0.6591\n",
            "Epoch [4/40], Train loss: 2.4626, Training Accuracy: 0.7359,  Validation loss: 2.5031, Validation Accuracy: 0.7086\n",
            "Epoch [5/40], Train loss: 2.4740, Training Accuracy: 0.7486,  Validation loss: 2.5071, Validation Accuracy: 0.7204\n",
            "Epoch [6/40], Train loss: 2.3818, Training Accuracy: 0.7521,  Validation loss: 2.5086, Validation Accuracy: 0.7255\n",
            "Epoch [7/40], Train loss: 2.4375, Training Accuracy: 0.7530,  Validation loss: 2.5077, Validation Accuracy: 0.7238\n",
            "Epoch [8/40], Train loss: 2.4325, Training Accuracy: 0.7855,  Validation loss: 2.5167, Validation Accuracy: 0.7606\n",
            "Epoch [9/40], Train loss: 2.4403, Training Accuracy: 0.7964,  Validation loss: 2.4991, Validation Accuracy: 0.7626\n",
            "Epoch [10/40], Train loss: 2.3314, Training Accuracy: 0.8328,  Validation loss: 2.5049, Validation Accuracy: 0.7963\n",
            "Epoch [11/40], Train loss: 2.4368, Training Accuracy: 0.8541,  Validation loss: 2.4976, Validation Accuracy: 0.8429\n",
            "Epoch [12/40], Train loss: 2.3299, Training Accuracy: 0.9174,  Validation loss: 2.4968, Validation Accuracy: 0.8739\n",
            "Epoch [13/40], Train loss: 2.2973, Training Accuracy: 0.9214,  Validation loss: 2.4963, Validation Accuracy: 0.8727\n",
            "Epoch [14/40], Train loss: 2.3745, Training Accuracy: 0.9213,  Validation loss: 2.4946, Validation Accuracy: 0.8760\n",
            "Epoch [15/40], Train loss: 2.2981, Training Accuracy: 0.9215,  Validation loss: 2.4955, Validation Accuracy: 0.8776\n",
            "Epoch [16/40], Train loss: 2.3422, Training Accuracy: 0.9218,  Validation loss: 2.4968, Validation Accuracy: 0.8792\n",
            "Epoch [17/40], Train loss: 2.2699, Training Accuracy: 0.9285,  Validation loss: 2.4967, Validation Accuracy: 0.9079\n",
            "Epoch [18/40], Train loss: 2.2484, Training Accuracy: 0.9612,  Validation loss: 2.4961, Validation Accuracy: 0.9158\n",
            "Epoch [19/40], Train loss: 2.2800, Training Accuracy: 0.9615,  Validation loss: 2.4935, Validation Accuracy: 0.9194\n",
            "Epoch [20/40], Train loss: 2.2635, Training Accuracy: 0.9616,  Validation loss: 2.4939, Validation Accuracy: 0.9167\n",
            "Epoch [21/40], Train loss: 2.2633, Training Accuracy: 0.9616,  Validation loss: 2.4941, Validation Accuracy: 0.9167\n",
            "Epoch [22/40], Train loss: 2.2505, Training Accuracy: 0.9752,  Validation loss: 2.2583, Validation Accuracy: 0.9722\n",
            "Epoch [23/40], Train loss: 2.2476, Training Accuracy: 0.9996,  Validation loss: 2.2500, Validation Accuracy: 0.9735\n",
            "Epoch [24/40], Train loss: 2.2490, Training Accuracy: 1.0000,  Validation loss: 2.2498, Validation Accuracy: 0.9746\n",
            "Epoch [25/40], Train loss: 2.2475, Training Accuracy: 0.9999,  Validation loss: 2.2501, Validation Accuracy: 0.9719\n",
            "Epoch [26/40], Train loss: 2.2481, Training Accuracy: 1.0000,  Validation loss: 2.2498, Validation Accuracy: 0.9748\n",
            "Epoch [27/40], Train loss: 2.2476, Training Accuracy: 1.0000,  Validation loss: 2.2485, Validation Accuracy: 0.9751\n",
            "Epoch [28/40], Train loss: 2.2473, Training Accuracy: 1.0000,  Validation loss: 2.2485, Validation Accuracy: 0.9792\n",
            "Epoch [29/40], Train loss: 2.2503, Training Accuracy: 0.9999,  Validation loss: 2.2474, Validation Accuracy: 0.9751\n",
            "Epoch [30/40], Train loss: 2.2474, Training Accuracy: 1.0000,  Validation loss: 2.2477, Validation Accuracy: 0.9725\n",
            "Epoch [31/40], Train loss: 2.2479, Training Accuracy: 1.0000,  Validation loss: 2.2475, Validation Accuracy: 0.9785\n",
            "Epoch [32/40], Train loss: 2.2473, Training Accuracy: 1.0000,  Validation loss: 2.2476, Validation Accuracy: 0.9763\n",
            "Epoch [33/40], Train loss: 2.2473, Training Accuracy: 1.0000,  Validation loss: 2.2475, Validation Accuracy: 0.9820\n",
            "Epoch [34/40], Train loss: 2.2472, Training Accuracy: 1.0000,  Validation loss: 2.2477, Validation Accuracy: 0.9785\n",
            "Epoch [35/40], Train loss: 2.2473, Training Accuracy: 1.0000,  Validation loss: 2.2487, Validation Accuracy: 0.9760\n",
            "Epoch [36/40], Train loss: 2.2474, Training Accuracy: 1.0000,  Validation loss: 2.2495, Validation Accuracy: 0.9734\n",
            "Epoch [37/40], Train loss: 2.2474, Training Accuracy: 1.0000,  Validation loss: 2.2478, Validation Accuracy: 0.9741\n",
            "Epoch [38/40], Train loss: 2.2473, Training Accuracy: 1.0000,  Validation loss: 2.2478, Validation Accuracy: 0.9793\n",
            "Epoch [39/40], Train loss: 2.2475, Training Accuracy: 0.9999,  Validation loss: 2.2472, Validation Accuracy: 0.9677\n",
            "Epoch [40/40], Train loss: 2.2472, Training Accuracy: 1.0000,  Validation loss: 2.2475, Validation Accuracy: 0.9767\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'status': 'ok', 'restart': True}"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    }
  ]
}